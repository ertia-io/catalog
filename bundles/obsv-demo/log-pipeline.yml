name: log-pipeline
enabled: true
version: 1.0.0
deployments:
  - name: otel-collector
    type: helm
    uri: git+::https://github.com/ertia-io/catalog/charts/monitoring/opentelemetry-collector-contrib/0.92.0@main
    enabled: true
    namespace: monitoring
    values:
      clusterRole:
        create: true
        rules:
         - apiGroups:
           - ''
           resources:
           - 'pods'
           - 'nodes'
           - 'endpoints'
           - 'services'
           verbs:
           - 'get'
           - 'list'
           - 'watch'

      presets:
        logsCollection:
          enabled: true
          includeCollectorLogs: false
        kubernetesAttributes:
          enabled: true
        kubeletMetrics:
          enabled: true
        kubernetesEvents:
          enabled: true
        hostMetrics:
          enabled: true
      config:
        receivers:
          otlp:
            protocols:
              grpc:
                endpoint: 0.0.0.0:4317
              http:
                endpoint: 0.0.0.0:4318
          prometheus:
            config:
              scrape_configs:
                - job_name: 'otel-ksm'
                  scrape_interval: 10s
                  scrape_timeout: 1s
                  static_configs:
                    - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8080']
                - job_name: 'hubble'
                  scrape_interval: 10s
                  static_configs:
                    - targets: ['hubble-metrics.kube-system.svc.cluster.local:9965']
                - job_name: 'kubernetes-service-endpoints'
                  scrape_interval: 10s
                  scrape_timeout: 1s
                  kubernetes_sd_configs:
                  - role: endpoints

                  relabel_configs:
                  - source_labels: [__meta_kubernetes_service_annotation_se7entyse7en_prometheus_scrape]
                    action: keep
                    regex: true
                  - source_labels: [__meta_kubernetes_service_annotation_se7entyse7en_prometheus_scheme]
                    action: replace
                    target_label: __scheme__
                    regex: (https?)
                  - source_labels: [__meta_kubernetes_service_annotation_se7entyse7en_prometheus_path]
                    action: replace
                    target_label: __metrics_path__
                    regex: (.+)
                  - source_labels: [__address__, __meta_kubernetes_service_annotation_se7entyse7en_prometheus_port]
                    action: replace
                    target_label: __address__
                    regex: ([^:]+)(?::\d+)?;(\d+)
                    replacement: $1:$2
                  - source_labels: [__meta_kubernetes_namespace]
                    action: replace
                    target_label: kubernetes_namespace
                  - source_labels: [__meta_kubernetes_service_name]
                    action: replace
                    target_label: kubernetes_service
                  - source_labels: [__meta_kubernetes_pod_name]
                    action: replace
                    target_label: kubernetes_pod

        processors:
          memory_limiter: null
          batch:
            send_batch_size: 100000
            timeout: 5s
          k8sattributes:
            auth_type: "serviceAccount"
            passthrough: false
            filter:
              node_from_env_var: KUBE_NODE_NAME
            extract:
              metadata:
                - k8s.pod.name
                - k8s.pod.uid
                - k8s.deployment.name
                - k8s.namespace.name
                - k8s.node.name
                - k8s.pod.start_time
              labels:
                - tag_name: app.label.component
                  key: app.kubernetes.io/component
                  from: pod
            pod_association:
              - sources:
                - from: resource_attribute
                  name: k8s.pod.ip
              - sources:
                - from: resource_attribute
                  name: k8s.pod.uid
              - sources:
                - from: connection
          resourcedetection:
            detectors: [env, system, gcp, ec2, azure]
            timeout: 2s
          transform:
            error_mode: propagate
            log_statements:
            - conditions:
              - IsMatch(body, "^\\{")
              context: log
              statements:
              - merge_maps(cache, ParseJSON(body), "upsert")
              - set(severity_text, "") where cache["level"] == nil
              - set(severity_text, ConvertCase(cache["level"],"upper")) where cache["level"] != nil
              - set(trace_id.string, String(cache["TRACE_ID"]))
              - set(span_id.string, String(cache["SPAN_ID"]))
              - set(time, cache["time"])
              - set(instrumentation_scope.name,cache["service.name"])
              - set(instrumentation_scope.version,cache["service.version"])

              # Custom
              - set(attributes["team"],cache["team"])
              - set(attributes["stories"],cache["stories"])
              - set(attributes["fraud"],cache["fraud"])
              - set(attributes["incident"],cache["incident"])
            - conditions:
              - IsMatch(body, "@m")
              context: log
              statements:
              - merge_maps(cache, ParseJSON(body), "upsert")
              - set(severity_text, "INFO")
              - set(severity_text, ConvertCase(cache["@l"],"upper")) where cache["@l"] != nil
              - set(trace_id.string, String(cache["@tr"]))
              - set(span_id.string, String(cache["@sp"]))
              - set(time, cache["@t"])
              - set(attributes["event_id"], cache["EventId"]["Id"])
              - set(attributes["elapsed"], cache["ElapsedMilliseconds"])
            - conditions:
              - severity_text == ""
              context: log
              statements:
              - set(severity_text, "INFO") where IsString(body) and IsMatch(body,
                "\\sstdout\\s")
              - set(severity_text, "ERROR") where IsString(body) and IsMatch(body,
                "\\sstderr\\s")
            - conditions:
              - severity_text == ""
              context: log
              statements:
              - set(severity_text, "INFO") where IsString(body) and IsMatch(body,
                "\\sINFO\\s")
              - set(severity_text, "WARN") where IsString(body) and IsMatch(body,
                "\\sWARN\\s")
              - set(severity_text, "ERROR") where IsString(body) and IsMatch(body,
                "\\sERROR\\s")
              - set(severity_text, "DEBUG") where IsString(body) and IsMatch(body,
                "\\sDEBUG\\s")
              - set(severity_text, "TRACE") where IsString(body) and IsMatch(body,
                "\\sTRACE\\s")
              - set(severity_text, "FATAL") where IsString(body) and IsMatch(body,
                "\\sFATAL\\s")
            - conditions:
              - severity_text == ""
              context: log
              statements:
              - set(severity_text, "UNKNOWN")
            - conditions:
              - severity_number == 0
              context: log
              statements:
              - set(severity_number, SEVERITY_NUMBER_TRACE) where severity_text=="TRACE"
              - set(severity_number, SEVERITY_NUMBER_DEBUG) where severity_text=="DEBUG"
              - set(severity_number, SEVERITY_NUMBER_INFO) where severity_text=="INFO"
              - set(severity_number, SEVERITY_NUMBER_WARN) where severity_text=="WARN"
              - set(severity_number, SEVERITY_NUMBER_ERROR) where severity_text=="ERROR"
              - set(severity_number, SEVERITY_NUMBER_FATAL) where severity_text=="FATAL"
            - conditions:
              - IsMatch(body, "^\\{")
              context: log
              statements:
              - merge_maps(cache, ParseJSON(body), "upsert")
              - set(body, cache["message"])


        exporters:
          clickhouse:
            endpoint: clickhouse://clickhouse.monitoring.svc.cluster.local:9000/otel?dial_timeout=10s&compress=lz4&secure=false
            username: default
            password: "{{ .Secret \"clickhouse.monitoring.password\" }}"
            database: otel
            ttl: 12h
            logs_table_name: otel_logs
            traces_table_name: otel_traces
            metrics_table_name: otel_metrics
            timeout: 5s
            retry_on_failure:
              enabled: true
              initial_interval: 5s
              max_interval: 30s
              max_elapsed_time: 300s
#          debug:
#            verbosity: detailed
        extensions:
           # The health_check extension is mandatory for this chart.
           # Without the health_check extension the collector will fail the readiness and liveliness probes.
           # The health_check extension can be modified, but should never be removed.
          health_check: {}
          memory_ballast: {}
        service:
          extensions:
            - health_check
            - memory_ballast
          pipelines:
            metrics:
              exporters:
                - clickhouse
              processors:
                - memory_limiter
                - batch
                - k8sattributes
                - resourcedetection
              receivers:
                - otlp
                - prometheus
            traces:
              exporters:
                - clickhouse
              processors:
                - memory_limiter
                - batch
              receivers:
                - otlp

            logs:
              exporters:
                - clickhouse
#                - debug
              processors:
                - transform
                - memory_limiter
                - batch
                - k8sattributes
                - resourcedetection
              receivers:
                - otlp
      image:
        # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
        repository: otel/opentelemetry-collector-contrib
        pullPolicy: IfNotPresent
        # Overrides the image tag whose default is the chart appVersion.
        tag: ""
      mode: daemonset
      service:
        enabled: true
        type: ClusterIP
